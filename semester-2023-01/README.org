Group meeting schedule, Spring 2023

Time 2:30-3:30pm on Thursday afternoons in [[https://nauedu-prod.modolabs.net/campus_map/map/index?state=detail&feed=campus_buildings_2021_v5&id=d2bb1e0a-4b88-5944-9466-68744bdcd981][Building 90]] (SICCS), room 224, 

What to present?
1. your research.
2. somebody else's paper.
3. some useful software.

- Jan 19: Toby, Title: [[file:HOCKING-berkeley-pres-jan-2023.pdf][Cross-validation for training and testing co-occurence network inference algorithms]]. Can we evaluate Chiquet's algorithm using cross-validation? [[https://pln-team.github.io/slideshow/slides#63][PLNmodels slides]], [[https://proceedings.mlr.press/v97/chiquet19a.html][ICML'19 paper]]. 
- Jan 26: Jadon
- Feb 2: Daniel
- Feb 9: Cam
- Feb 16: Austin
- Feb 23: Toby
- Mar 2: Jadon
- Mar 9: Daniel
- Mar 16: Spring Break!
- Mar 23: Cam
- Mar 30: Austin
- Apr 6: Toby
- Apr 13: Jadon
- Apr 20: Daniel
- Apr 27: Cam
- May 4: Austin
- May 11: Toby JSM talk. Title: Efficient line search optimization of penalty functions in supervised changepoint detection. Abstract: Receiver Operating Characteristic (ROC) curves are commonly used in binary classification, and can also be used to evaluate learned penalty functions in the context of supervised changepoint detection. Since the Area Under the Curve (AUC) is a piecewise constant function of the predicted values, it can not be directly optimized by gradient descent. Recently we showed that minimizing a piecewise linear surrogate loss, AUM (Area Under Min of false positives and false negatives), results in maximizing AUC. In this talk we propose a new algorithm for AUM minimization, which exploits the piecewise linear structure to efficiently compute an exact line search, for every step of gradient descent. Because the exact line search is quadratic time in the worst case, we additionally propose an approximate line search which is log-linear time in the worst case (asymptotically the same as a constant step size). Our empirical results show that the proposed algorithm is more computationally efficient than other variants of gradient descent (constant step size, line search using grid search, etc).
